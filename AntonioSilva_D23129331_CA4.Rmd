---
title: "MATH 9102 - Probability and Statistical Inference Assignment - 4"
author: "Antonio Silva (D23129331@mytudublin.ie)"
output:
  html_document:
    toc: true
    toc_depth: 2
    mathjax: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
---

```{r, echo = FALSE, results = "hide"}
needed_packages <- c("stats", "lm.beta", "stargazer", "ggplot2", "ppcor", "car", "nortest", "dplyr", "tidyverse", "lmtest", "pROC")                      
not_installed <- needed_packages[!(needed_packages %in% installed.packages()[ , "Package"])]    

if(length(not_installed)) install.packages(not_installed, repos ="http://cran.us.r-project.org")                              
library(stats, quietly = T)
library(ggplot2, quietly = T)
library(lm.beta, quietly = T)
library(stargazer, quietly = T)
library(ppcor, quietly = T)
library(car, quietly = T)
library(nortest, quietly = T)
library(dplyr, quietly = T)
library(lmtest, quietly = T)
library(pROC, quietly = T)
```

# Question 1 - Linear regression with dummy variable [5 marks]
Consider the dataset "weatherhistory.csv".

 a) Describe dependent variable pressure and independent variable temperature;
 b) Explore the relationship between pressure and temperature;
 c) Build a linear model considering temperature and pressure;
 d) Identify a dummy variable and build extended model considering dummy variable;
 e) Report your findings;

## Answer
### a) Describe dependent variable pressure and independent variable temperature.
Load the dataset

```{r, echo=TRUE}
weatherHistory <- read.csv("weatherHistory.csv")
str(weatherHistory)
```

```{r, echo=TRUE}
head(weatherHistory)
```

```{r, echo=TRUE}
colnames(weatherHistory)
```
```{r, echo=TRUE}
ggplot(weatherHistory, aes(y = temperature)) +
  geom_boxplot(fill = "lightblue", colour = "darkblue") +
  labs(title = "Boxplot of Temperature", y = "Temperature") +
  theme_minimal()
```

```{r, echo=TRUE}
ggplot(weatherHistory, aes(y = pressure)) +
  geom_boxplot(fill = "lightblue", colour = "darkblue") +
  labs(title = "Boxplot of Temperature", y = "Temperature") +
  theme_minimal()
```

```{r, echo=TRUE}
ggplot(weatherHistory, aes(x = temperature)) +
  geom_histogram(bins = 60, fill = "skyblue", color = "black") + # You can adjust the number of bins
  labs(title = "Histogram of Temperature", x = "Temperature (°C)", y = "Frequency") +
  theme_minimal()
```

```{r, echo=TRUE}
ggplot(weatherHistory, aes(x = pressure)) +
  geom_histogram(bins = 200, fill = "lightgreen", color = "black") + # Adjust the number of bins as needed
  labs(title = "Histogram of Pressure", x = "Pressure (hPa)", y = "Frequency") +
  theme_minimal()
```

```{r, echo=TRUE}
psych::describe(weatherHistory[, c("temperature", "pressure")])
```

Temperature has the mean 11.93 and the a standard deviation of 9.55. With this
standard deviation showing significant fluctuations in the temperature.
With the kurtosis of -0.57 indicates a distribution flatter than a normal 
distribution. We also have 0.03 of standard error of the mean that the measure 
is precise.
We can conclude that temperature is a symmetric and a flatter variable.

Pressure data in the other hand shows a skew data with a lot of zero values.
Hit has a kurtosis of 69.26 indicate thick tails and a skew of -8.42 indicating
the data is concentrated in the right side of the mean. 


Testing the temperature for normality

```{r, echo=TRUE}
ggplot(weatherHistory, aes(sample=temperature)) + stat_qq()
```

```{r, echo=TRUE}
ad.test(weatherHistory$temperature)

ad.test(weatherHistory$pressure)
```

We can also see that neither temperature or pressure follows a normal 
distribution.

### b) Explore the relationship between pressure and temperature.

To explore the relationship between pressure and temperature.

```{r, echo=TRUE}
ggplot(weatherHistory, aes(x = temperature, y = pressure)) +
  geom_point(alpha = 0.5) +  # using some transparency for points
  geom_smooth(method = "lm", se = FALSE, color = "blue", formula = y ~ x) +  
  labs(title = "Temperature vs. Pressure",
       x = "Temperature",
       y = "Pressure") +
  theme_minimal()
```
It seems that the pressure outliers are affecting the relationship between 
temperature and pressure. Let's remove them e see the correlation.

```{r, echo=TRUE}
weatherHistoryWithPressure <- weatherHistory %>%  
    filter(pressure != 0) %>%  
    filter(!is.na(temperature) & !is.na(pressure))


ggplot(weatherHistoryWithPressure, aes(x = temperature, y = pressure)) +
  geom_point(alpha = 0.5) +  # using some transparency for points
  geom_smooth(method = "lm", se = FALSE, color = "blue", formula = y ~ x) + 
  labs(title = "Temperature vs. Pressure",
       x = "Temperature",
       y = "Pressure") +
  theme_minimal()
```

Let's also do a Pearson's correlation between the variables.

```{r, echo=TRUE}
cor.test(weatherHistoryWithPressure$temperature, weatherHistoryWithPressure$pressure)
```
The scatterplot suggest that we have a non-linear relationship between 
temperature and pressure. So doing a linear regression model will not capture
or explain the relationship between them.

The correlation coefficient from the Pearson's test indicates a negative 
relationship of 0.31. So it means when the temperature increases the pressure
decreases.

The p-value is also too low (2.2e-16) suggesting the relationship is statistical
significant.

### c) Build a linear model considering temperature and pressure.

First let's create the model.

```{r, echo=TRUE}
lmTemperaturePressure <-  lm(pressure ~ temperature, 
                             data = weatherHistoryWithPressure)
anova(lmTemperaturePressure)
```

```{r, echo=TRUE}
summary(lmTemperaturePressure)
```

Standartize co-efficients to get the result expressed in standart deviations.
```{r, echo=TRUE}
lm.beta(lmTemperaturePressure)  
```

```{r, echo=TRUE}
suppressWarnings(stargazer(lmTemperaturePressure, type="text"))
```
So we got the following results:

- For every 1 degree increase in temperature the pressure decreases 0.253 units.
- The p value is 0.01 who give us 99% confidence level. Also our F statistic
is 10,150.310. Suggesting that the temperature is a good predictor of pressure.
- R squared is 0.096 meaning that 9.6% of the pressure variance is explained
by the temperature.


```{r, echo=TRUE}
plot(residuals(lmTemperaturePressure), main = "Residuals of the Model",
     xlab = "Index", ylab = "Residuals")
abline(h = 0, col = "red")
```


```{r, echo=TRUE}
plot(lmTemperaturePressure)
```

Looking at the residuals we can see that there is a funneling effect 
showing a spread out as the fitted values increase suggesting a non-constant
variance. We get a bigger residuals for lower or higher pressures.

#### d) Identify a dummy variable and build extended model considering dummy variable.

Let's use the categorical variable precipType as a dummy variable for our model
 
```{r, echo=TRUE}
weatherHistoryDummy <- weatherHistory
weatherHistoryDummy$humidityScale <- cut(weatherHistory$windSpeed, 
                                     breaks=c(0, 0.25, 0.6, 1), 
                                     labels=c("dry", "comfortable", "humid"))
levels(weatherHistoryDummy$humidityScale)
table(weatherHistoryDummy$humidityScale)
```
 
So we have we possible values: dry, rain and snow.

Mapping them in our dataset.

```{r, echo=TRUE}
weatherHistoryDummy$dry <- ifelse(weatherHistoryDummy$humidityScale == "dry", 1, 0)
weatherHistoryDummy$comfortable <- ifelse(weatherHistoryDummy$humidityScale == "comfortable", 1, 0)
weatherHistoryDummy$humid <- ifelse(weatherHistoryDummy$humidityScale == "humid", 1, 0)

table(weatherHistoryDummy[, c("humidityScale")])
table(weatherHistoryDummy[, c("dry")])
table(weatherHistoryDummy[, c("comfortable")])
table(weatherHistoryDummy[, c("humid")])
```

Lets consider the humid dummy variable and how it influenciates the pressure.

```{r, echo=TRUE}
lmTempPresWithDummy <- lm(pressure ~ temperature + humid, data = weatherHistoryDummy)
```

### e) Report your findings.
 
```{r, echo=TRUE}
summary(lmTempPresWithDummy)
```

```{r, echo=TRUE}
lm.beta(lmTempPresWithDummy)  
```

```{r, echo=TRUE}
suppressWarnings(stargazer(lmTempPresWithDummy, type="text"))
```

If we consider the humidity as dummy variable we get the model equaltion:
$$ pressure = 1010.012 - 0.085 \times temperature + 6.052 \times humid  $$

```{r, echo=TRUE}
plot(lmTempPresWithDummy)
```

Conclusions: 
 
 - The pressure is 1010.012 when the temperature and humidity is 0;
 - The pressure drops 0.085 per degree increase in temperature;
 - When the weather is humid the pressure increases 6.052;
 - R-squared is less than 0.001 indicating that the model explain less than 1%
   of the variance to the pressure;
 - A negative adjusted R-squared suggests that the model fits the data worse than
   just the mean of the dependent variable. 
 -  F-statistic is 0.4302 with a p-value of 0.6505, indicating that the model
    is not statistically significant.
    

# Question 2 - Multiple Linear Regression [6 marks]
Consider the dataset "weatherhistory.csv".

a) Explore the relationship between pressure and windspeed;
b) Build a linear model considering (windspeed, humidity, temperature) and pressure;
c) Assess how model meets key assumptions of linear regression;
d) Investigate a differential effect by adding dummy variable;
e) Investigate interaction between effect for windspeed and dummy variable;
f) Report your findings.

## Answer
### a) Explore the relationship between pressure and windspeed

First we create a scatterplot to visualize the correlation between pressure
and windspeed.

```{r, echo=TRUE}
ggplot(weatherHistory, aes(x = windSpeed, y = pressure)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", color = "blue", formula="y ~ x") +
  labs(title = "Pressure vs Wind Speed",
       x = "Wind Speed",
       y = "Pressure") +
  theme_minimal()
```

We have a lot of points with pressure equals 0. So let's remove the outliers.

```{r, echo=TRUE}
ggplot(weatherHistoryWithPressure, aes(x = windSpeed, y = pressure)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", color = "blue", formula="y ~ x") +
  labs(title = "Pressure vs Wind Speed",
       x = "Wind Speed",
       y = "Pressure") +
  theme_minimal()
```

Let's also do a Pearson correaltion test

```{r, echo=TRUE}
cor.test(weatherHistoryWithPressure$pressure, weatherHistoryWithPressure$windSpeed)
```
Considerations;
 
- The regression line, shown in blue, indicates a negative linear relationship
   between wind speed and pressure. As wind speed increases, pressure tends to
   decrease;
- The data points are concentrated for lower wind speeds suggesting 
  higher variance for higher wind speeds. Also its uncommon wind speeds higher
  than 40;
- The negative correlation of 0.25 suggest indicates a week relationship 
  between both variables;
- The p-value of 2.2e-16 for a confidence level of 95% suggests that
  the correlation is statistical significant.

### b) Build a linear model considering (windspeed, humidity, temperature) and pressure

```{r, echo=TRUE}
modelPressure <- lm(pressure ~ windSpeed + humidity +  temperature, data = weatherHistoryWithPressure)
summary(modelPressure)
```

Standartize co-efficients to get the result expressed in standart deviations.
```{r, echo=TRUE}
lm.beta(modelPressure)
suppressWarnings(stargazer(modelPressure, type="text"))
```

We got the model equation:
$$ pressure = 1037.444 - 0.377 \times windSpeed - 15.253 \times humidity - 0.448 \times temperature $$
We can conclude the following:

- Intercept 1037.444. Value expected when all predictors are zt zero.
- All predictors are negative indicating then when they increase the pressure
  will drop for every unit each (0.377 windSpeed, 15.256 humidity and 0.448 temperature);
- The humidity is the less influential in the pressure because it ranges between 0 and 1;
- All the variables have a p-value lower tha 0.01 meaning that we have a confidence level bigger than 99% that they're statistical significant;
- The $R^2$ and $Ajusted\ R^2$ are 0.24 meaning that this model explain 24% of the pressure variance;
- We have a residual standart error of approximately 6.779. Considering that the pressure ranges are bigger than 1000 is a small error;
- The F Statistic 10,037.910 is a  high and significant (p < 0.01) value confirm that is a statistic model.

### c) Assess how model meets key assumptions of linear regression.

#### Linear relationship between the response variable and the predictors

```{r, echo=TRUE}
linearity <- weatherHistoryWithPressure |> mutate(yhat = fitted(modelPressure), res = residuals(modelPressure))

ggplot(linearity, aes(x = yhat, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  geom_smooth(method = "gam", color = "blue", se = FALSE, formula = y ~ s(x, bs = "cs")) +
  labs(title = "Fitted vs Residuals",x = "Fitted",y = "Residuals")
```

```{r, echo=TRUE}
ggplot(linearity, aes(x = windSpeed, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  geom_smooth(method = "gam", color = "blue", se = FALSE, formula = y ~ s(x, bs = "cs")) +
  labs(title = "WindSpeed vs Residuals",x = "WindSpeed",y = "Residuals")
```

```{r, echo=TRUE}
ggplot(linearity, aes(x = humidity, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  geom_smooth(method = "gam", color = "blue", se = FALSE, formula = y ~ s(x, bs = "cs")) +
  labs(title = "humidity vs Residuals",x = "humidity",y = "Residuals")
```

```{r, echo=TRUE}
ggplot(linearity, aes(x = temperature, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  geom_smooth(method = "gam", color = "blue", se = FALSE, formula = y ~ s(x, bs = "cs")) +
  labs(title = "temperature vs Residuals",x = "temperature",y = "Residuals")
```

We got the following results:  

- The first chart shows the Fitted Values vs Residuals. It shows a curve
  in the tails (lower and higher pressures), not suggesting that the relationship
  between the predictors and the dependent variable is not linear.

- The chart for the WindSpeed is fairly linear until we get the 30 Km/h but 
  after that we get a positive curve not suggesting a linear relation for higher windspeeds
  with the variance increasing.
  
- In the other side we have the humidity. The variance decreases with lower humidity values (<0.25)
  and after is fairly constant suggesting a linear relationship for humidity >= 0.25.
  
- The temperature variable agains the residuals show a sightly curve around 
  the 0 line. Show we can say that the temperature is somewhat linear.
  
The result is that *this model is not a linear model*. We can improve it removing 
the outliers or do some kind of transformation.

#### Independence of errors

```{r, echo=TRUE}
plot(resid(modelPressure), type = 'l', main = "Residuals Plot",
     xlab = "Index", ylab = "Residuals")
abline(h = 0, col = "red")
```

The Residuals Plot isplays the residuals from your linear regression model 
plotted against their index, 
which represents the order in which data were collected (you can confirm
by the time of the data tha is in order).

As we can see the model does not presents obvious bias estimating the
dependent variable.

We can assume that the *model is independent of errors*.

#### Homoscedasticity

The variance of the residuals should remain constant across all levels of the 
independent variables. 

By the previous Residuals Plots we see that they present a funneling shape
for all variables suggesting that the *model exhibits heteroscedasticity*.
To confirm we can do Breusch-Pagan test.

```{r, echo=TRUE}
bptest(modelPressure)
```
Given the result of the test we can conclude that with a small p-value (2.2e-16)
we have strong evidence that the *variance of the residuals is not constant*.

#### Normality of Errors

Residuals should be approximately normally distributed. To verify we will do
the histogram for the residuals

```{r, echo=TRUE}
ggplot(linearity, aes(x = res)) +
  geom_histogram(aes(y = after_stat(density)), bins = 50) +
  ggtitle("Histogram of Residuals") +
  xlab("Residuals") +
  ylab("Density") +
  stat_function(fun = dnorm, args = list(mean = mean(linearity$res), sd = sd(linearity$res)), color = "blue", linewidth = 1) +
  theme_minimal()
```

```{r, echo=TRUE}
qqnorm(linearity$res, main = "Q-Q Plot of Residuals")
qqline(linearity$res, col = "red")
```

```{r, echo=TRUE}
ad.test(linearity$res)
```
By the qqplot and the result of Anderson-Darling normality test we can conclude
that the *residuals don't not follow a normal distribution*.

```{r, echo=TRUE}
ad.test(linearity$res)
```

#### No Multicollinearity among Predictors

To measure how much the variance of an estimated regression coefficient 
increases if the predictors are correlated.

So if the Vif results can be interpreted based on the results per variable. If VIF equals:

- 1: that there is no correlation between a given predictor with others;
- >= 1 && <= 5: Moderate correlation but no severe;
- > 5: Higly correlated.

```{r, echo=TRUE}
vif(modelPressure)
```

```{r, echo=TRUE}
vif(modelPressure)
```


Based on our results or predictors are *Lightly correlate between them* 
not *affecting our predictive model*. In other words: 
*each coefficient produced by the regression model is likely reliable*.

#### No Perfect Multicollinearity

```{r, echo=TRUE}
cor(weatherHistoryWithPressure[c("windSpeed", "humidity", "temperature")])
```
Looking at the correlation matrix we only have one moderate negative correlation,
that is humidity with temperature (-0.633). This value is significant but not
indicative of high multicollinearity.

So we can conclude that *there is no Perfect Multicollinearity in our model*.

### d) Investigate a differential effect by adding dummy variable

First lets create a dummy variable. We consider to create a variable to verify 
if its rainning or not.

```{r, echo=TRUE}
weatherHistoryWithPressure$rain <- ifelse(weatherHistoryWithPressure$precipType == "rain", 1, 0)
table(weatherHistoryWithPressure$rain)
```

Next we have the both models. Results will be discuss later.

```{r, echo=TRUE, results='asis'}
modelPressureSimple <- lm(pressure ~ windSpeed + humidity +  temperature , data = weatherHistoryWithPressure)
lm.beta(modelPressureSimple)
modelPressureDummy <- lm(pressure ~ windSpeed + humidity +  temperature + rain , data = weatherHistoryWithPressure)
lm.beta(modelPressureDummy)
summary(modelPressureSimple)
summary(modelPressureDummy)
```
### e) Investigate an interaction effect for windspeed and dummyvariable

Adding the interaction we need to add the term `windSpeed:rain` to understand
the pressure correlates with windSpeed for rainy and non-rainy days.
Conclusions will be done later.

```{r, echo=TRUE, results='asis'}
modelPressureDummyInteraction <- lm(pressure ~ windSpeed + humidity +  temperature + rain  + windSpeed:rain, data = weatherHistoryWithPressure)
lm.beta(modelPressureDummyInteraction)
summary(modelPressureDummyInteraction)
```

### f) Report your findings

```{r, echo=TRUE}
suppressWarnings(stargazer(modelPressureSimple, modelPressureDummy, modelPressureDummyInteraction, type="text",
                           title= "Regression Results", align = T, 
                           column.labels = c("Pressure", "Pressure with Dummy", "Pressure with Dummy and Interaction")))
```

Comparing the both models we find the following conclusions:

- The dummy variable rain its interaction with windSpeed increases the 
  explainability of the pressure. 
- The addition of rain and its interaction with windSpeed increase the 
  explainability 0.8%. We can tell that 
  *the effect of windSpeed in pressure can differ on rainy or non-rainy days*;
- The residuals are sightly lower in the latest model than the first without
  the dummy variable. It means the inclusion made the prediction more accurate.

# Question 3 - Logistic regression [4 marks]
Consider the dataset "heartfailure.csv".

 a) Build a model considering diabetes as predictor;
 b) Calculate and analyze odds ratio of the model;
 c) Extend the model by considering variable age. 
   (Convert the age into categorical data, 
        if age < 55 , category 1; 
        age is between 55 and 68, category 2; 
        otherwise category 3
        );
 d) Report your findings;

## Answer
### a) Build a model considering diabetes as predictor
Load the dataset and analyze the structure.

```{r, echo=TRUE}
heartFailure <- read.csv("heartfailure.csv")
str(heartFailure)
```
Either diabetes predictor and DEATH_EVENT are *Binary Variables*. In the structure
they are represent as a int, so they need to be corrected.

```{r, echo=TRUE}
heartFailure$diabetes <- factor(heartFailure$diabetes, levels = c(0, 1))
heartFailure$DEATH_EVENT <- factor(heartFailure$DEATH_EVENT, levels = c(0, 1))
str(heartFailure)
```
Describing both variables we got:
```{r, echo=TRUE}
table(heartFailure$diabetes)
table(heartFailure$DEATH_EVENT)

ggplot(heartFailure, aes(x = factor(diabetes), fill = factor(DEATH_EVENT))) +
  geom_bar(position = "fill") +
  scale_x_discrete(labels = c("No", "Yes")) +
  scale_fill_manual(values = c("darkgreen", "darkred"), labels = c("Survived", "Died")) +
  labs(title = "Proportion of Deaths by Diabetes",
       x = "Diabetes", y = "Proportion", fill = "Outcome")
```
By the chart proportion histogram we have two insights:

- Both groups has more survivals than deaths. 
- The proportion of death between Deaths and Non-Diabets are similar suggesting
that diabetes is not a good predictor for the outcome. 

Modelling the 
```{r, echo=TRUE}
modelDiabetes <- glm(DEATH_EVENT ~ diabetes, data = heartFailure, family = binomial())

summary(modelDiabetes)

suppressWarnings(stargazer(modelDiabetes, type="text"))
```

So we got the following model equation:

$$
  p(x) = \frac{1}{e^{-(intercept + b_{diabetes} \times diabetes)}} = \frac{1}{e^{-(-0.745333 - 0.008439 \times diabetes)}} = \frac{1}{e^{(0.745333 + 0.008439 \times diabetes)}}
$$

Because diabetes is a binary variable we have 2 possible values:

$$
  p(0) = \frac{1}{e^{0.745}} \\ 
  p(1) = \frac{1}{e^{(0.745 + 0.008)}}
$$

We evaluate the model doing the 3 tests:
- Likelihood Ratio Test: Explain the variable explanability of the model. 
  Good to compare if the curve fits better compared with other models with other variables;
- Verify the Confusion Matrix. Good to measure the performance and accuracy;
- ROC and AUC curve.The ROC curve visually shows the trade-off between
  sensitivity and specificity for every possible cut-off.
  The AUC value quantifies the overall ability of the test to discriminate
  between the individuals having the outcome and those not having the outcome.

Likelihood ratio test
```{r, echo=TRUE}
lrtest(modelDiabetes)
```


```{r, echo=TRUE}
lrtest(modelDiabetes)
```

```{r, echo=TRUE}
predicted_probs <- predict(modelDiabetes, type = "response")
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)
actual_classes <- heartFailure$DEATH_EVENT

confusion_matrix <- suppressWarnings(caret::confusionMatrix(factor(predicted_classes), factor(actual_classes), positive = "1"))
print(confusion_matrix)
```
```{r, echo=TRUE}
roc_curve <- roc(heartFailure$DEATH_EVENT, fitted(modelDiabetes))
plot(roc_curve, main="ROC Curve for Diabetes Model", col="blue", print.auc = T)
```


Conclusions:

- Intercept of -0.745 with standard error of 0.162. This negative value
indicates the absence of diabetes. The p-value is 4.37e-06 meaning that 
this value is statically significant for a confident level of 99.9%;
- Diabetes coefficient of -0.008430 with a standard error of 0.251;
  the p-value of 0.973 is almost 1 (much greater than 0.05)
  we can conclude that there is *no statically significant evidence* that
  diabetes has an effect on probability;
- The AUC is the same than the ROC curve meaning that diabetes is irrelevant 
  to predict heart failure;
- The confusion matrix shows that the model fails to identify true positives
  (sensitivity = 0) suggesting that *diabetes is not a good indicator*;
- The Likelihood ratio shows *no improvement adding the diabetes variable* to the model (no change compared with the reference value).

### b) Calculate and analyze odds ratio of the model

```{r, echo=TRUE}
coefDiabetes <- coef(modelDiabetes)["diabetes1"]
exp(coefDiabetes)
```

To calculate the odds ratio, we exponentiate the coefficient of diabetes. 
This coefficient represents the log-odds of diabetes influencing the outcome. 
The odds ratio itself quantifies the strength of the association between
diabetes and the occurrence of a DEATH_EVENT.

Because the Odds Ratio is close to 1 ($OR_{diabetes} \approx 1$)  this means the diabetes has 
 *little to no effect* predicting DEATH_EVENT.
 
This is confirmed our p-value 0.973 (Wald Test) that is that is not statistical
significant.

### c) Extend the model by considering variable age. 
    (Convert the age into categorical data, 
    - if age < 55 , category 1; 
    - age is between 55 and 68 category 2 ; 
    - otherwise category 3)

First we well create a new category and add it for our dataset.
```{r, echo=TRUE}
heartFailure$AGE_CAT <- cut(heartFailure$age, 
                            breaks = c(-Inf, 55, 68, Inf), 
                            labels = c("1", "2", "3"),
                            right = FALSE)

ggplot(heartFailure, aes(x = factor(AGE_CAT), fill = factor(DEATH_EVENT))) +
  geom_bar(position = "fill") +
  scale_x_discrete(labels = c("0-54", "55-67", "68+")) +
  scale_fill_manual(values = c("darkgreen", "darkred"), labels = c("Survived", "Died")) +
  labs(title = "Proportion of Deaths by Age",
       x = "Diabetes", y = "Proportion", fill = "Outcome")
```

Extend the model
```{r, echo=TRUE}
modelDeathDiabetesAge <- glm(DEATH_EVENT ~ diabetes + AGE_CAT, family = binomial(), data = heartFailure) 
summary(modelDeathDiabetesAge)

suppressWarnings(stargazer(modelDeathDiabetesAge, type="text"))
```



Likelihood ratio test
```{r, echo=TRUE}
lrtest(modelDeathDiabetesAge)
```
Confusion Matrix:
```{r, echo=TRUE}
predicted_probs_wage <- predict(modelDeathDiabetesAge, type = "response")
predicted_classes_wage <- ifelse(predicted_probs_wage > 0.5, 1, 0)
actual_classes_wage <- heartFailure$DEATH_EVENT

confusion_matrix_wage <- suppressWarnings(caret::confusionMatrix(factor(predicted_classes_wage), factor(actual_classes_wage), positive = "1"))
print(confusion_matrix_wage)
```

And the ROC Curve:
```{r, echo=TRUE}
roc_curve_wage <- roc(heartFailure$DEATH_EVENT, fitted(modelDeathDiabetesAge))
plot(roc_curve_wage, main="ROC Curve for Diabetes + Age Model", col="blue", print.auc = T)
```

Model we consider the reference level as $diabetes = 0$ and $AGE\_CAT = 1$:

$$
  p(x) = \frac{1}{e^{-(\beta_{intercept} + \beta_{diabetes} \times diabetes + \beta_{AGE\_CAT2} \times AGE\_CAT2 + \beta_{AGE\_CAT3} \times AGE\_CAT3)}} \\ 
  p(x) = \frac{1}{e^{-(−1.2695 + 0.1586 \times diabetes + 0.1893 \times AGE\_CAT2 + 1.1993 \times AGE\_CAT3)}} \\
  p(x) = \frac{1}{e^{1.2695 - 0.1586 \times diabetes - 0.1893 \times AGE\_CAT2 - 1.1993 \times AGE\_CAT3)}} 
$$

We can use the log form equivalent of this equation:
$$
log\left(\frac{p}{1 - p}\right) = \beta_{intercept} + \beta_{diabetes} \times diabetes + \beta_{AGE\_CAT2} \times AGE\_CAT2 + \beta_{AGE\_CAT3} \times AGE\_CAT3) \\
log\left(\frac{p}{1 - p}\right) = −1.2695 + 0.1586 \times diabetes + 0.1893 \times AGE\_CAT2 + 1.1993 \times AGE\_CAT3
$$


Conclusions:
- As we saw by the proportion histogram there is almost 50% of deaths in the 
   Category 3 of (age 68+). Age looks a good predictor for our model;
- Intercept coefficient for $diabetes = 0$ and $AGE\_CAT = 1$ is -1.2695.
  The probability $p = 2.79e-6$ which indicates a high significant level;
- The p-value of diabetes is $0.547029$ so diabetes is not statistical significant.
  The increase is minimal with a $\beta_{diabetes} = 0.1586$;
- The same can be said for AGE_CAT2 that has a p-value of $p-value = 0.553987$.
  Increase in the DEATH_EVENT is residual ($\beta_{AGE\_CAT2} = 0.1893$);
- *AGE_CAT3 (68 years and older) shows a substantial increase in the log odds of a DEATH_EVENT*.
  It has a ($\beta_{AGE\_CAT3} = 1.1993$) and p-value of $ (p = 0.000264)$ who makes it 
  statistical significant. This assumption is reasonable because as you get older
  you have more change of die.
- Likelihood ratio test shows that including diabetes and AGE_CAT in the model
  significantly improves the model's explanatory power compared to just using 
  the intercept. We know that we are considering only the AGE_CAT because
  we saw in the earlier that diabetes didn't improve the explainability of the model;
- In the confusion matrix we can see that we have a huge number of False Negatives (84)
  who contribute for a 67.56% of accuracy. We also have a P value of 0.5765 indicating
  that the test is not significant.
- We have a sensitivity of 12.5% is quite low but better than the model without the age.

### d) Report your finding

So let's compare both models
```{r, echo=TRUE}
suppressWarnings(stargazer(modelDiabetes, modelDeathDiabetesAge, 
                           type="text", 
                           title= "Regression Results",
                           column.labels = c("Diabetes", "Diabetes And Age"), 
                           align = T))

```

```{r, echo=TRUE}
par(mfrow=c(1,2))
plot(roc_curve, main="ROC Diabetes", col="blue", print.auc = T)
plot(roc_curve_wage, main="ROC Diabetes + Age", col="blue", print.auc = T)
```
  
Based on our previous findings and look at the ROC charts we can say that
the first model its the same than piking randomly who will die or not. So 
the predictor diabetes its the same than the reference model. When we add 
the variable age for the category 68+ we got a explainability of 12.5% what 
makes the age a good predictor but needs to be pair with other predictors
to increase the explainability.

We also got an improvement in the second model with an increase of the 
Likehood Ratio Test and (from -187.674 to -179.420) and a diminuition 
of the AIC (from 379.348 to 366.841).
